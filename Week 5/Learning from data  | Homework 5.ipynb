{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from data HW 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Error\n",
    "Consider a noisy target y = w∗T x + \u000f, where x ∈ R\n",
    "d\n",
    "(with the added coordinate\n",
    "x0 = 1), y ∈ R, w∗\n",
    "is an unknown vector, and \u000f is a noise term with zero mean and\n",
    "σ\n",
    "2 variance. Assume \u000f is independent of x and of all other \u000f’s. If linear regression\n",
    "is carried out using a training data set D = {(x1, y1), . . . ,(xN , yN )}, and outputs the\n",
    "parameter vector wlin, it can be shown that the expected in-sample error Ein with\n",
    "respect to D is given by:\n",
    "ED[Ein(wlin)] = σ\n",
    "2\n",
    "\u0012\n",
    "1 −\n",
    "d + 1\n",
    "N\n",
    "\u0013\n",
    "### 1)\n",
    "For σ = 0.1 and d = 8, which among the following choices is the smallest\n",
    "number of examples N that will result in an expected Ein greater than 0.008?\n",
    "\n",
    "- [a] 10\n",
    "- [b] 25\n",
    "- [c] 100\n",
    "- [d] 500\n",
    "- [e] 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the expression to plug in the different Ns\n",
    "\n",
    "def e_in(N):\n",
    "    return (0.01*((1-(float(9)/N))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010\n",
      "0.0064\n",
      "0.0091\n",
      "0.0098\n",
      "0.0099\n"
     ]
    }
   ],
   "source": [
    "X = [10, 25, 100, 500, 1000]\n",
    "\n",
    "for i in X:\n",
    "    print \"{0:0.4f}\".format(e_in(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell, the third value (100) is the first to cross the 0.008 limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Consider the nonlinear error surface E(u, v) = (uev − 2ve−u)\n",
    "\n",
    "### 4)\n",
    "We start at the point\n",
    "(u, v) = (1, 1) and minimize this error using gradient descent in the uv space. Use\n",
    "η = 0.1 (learning rate, not step size).\n",
    "\n",
    "4. What is the partial derivative of E(u, v) with respect to u, i.e., ∂E\n",
    "∂u ?\n",
    "[a] (uev − 2ve−u\n",
    ")\n",
    "2\n",
    "[b] 2(uev − 2ve−u\n",
    ")\n",
    "[c] 2(e\n",
    "v + 2ve−u\n",
    ")\n",
    "[d] 2(e\n",
    "v − 2ve−u\n",
    ")(uev − 2ve−u\n",
    ")\n",
    "[e] 2(e\n",
    "v + 2ve−u\n",
    ")(uev − 2ve−u\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, and that $\\frac{d}{dx} e^{-x} = -e^{-x}$, we quickly find that the answer is **e)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def E_uv(u, v):\n",
    "    return (u * np.exp(v) - 2 * v * np.exp(-u))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dE_du(u, v):\n",
    "    return 2*(np.exp(v) + 2* v * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dE_dv(u, v):\n",
    "    return 2 * (u * np.exp(v) - 2 * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_t fot t = 0 was 1.15950972997\n",
      "E_t fot t = 1 was 1.00740748296\n",
      "E_t fot t = 2 was 0.0990091216273\n",
      "E_t fot t = 3 was 0.00866064536281\n",
      "E_t fot t = 4 was 0.000181755791728\n",
      "E_t fot t = 5 was 1.29723984784e-06\n",
      "E_t fot t = 6 was 7.29152469846e-09\n",
      "E_t fot t = 7 was 4.00999789056e-11\n",
      "E_t fot t = 8 was 2.20168344841e-13\n",
      "E_t fot t = 9 was 1.20868339442e-15\n",
      "Tolerance reached after 10 iterations\n",
      "The current (u,v) is (0.0447362903978,0.0239587140991)\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent maafaakkaaas\n",
    "# In this here case we don't explicitly work with weights w(t), but rather u and v\n",
    "\n",
    "# Step 1 - Initialize w(0)\n",
    "\n",
    "u_0 = 1\n",
    "v_0 = 1\n",
    "\n",
    "# We set u and v in time t to this before the first iteration\n",
    "\n",
    "u_t = u_0\n",
    "v_t = v_0\n",
    "\n",
    "# We set the initial error value, and E_t to match this before any iterations of SGD\n",
    "\n",
    "E_0 = E_uv(u_0, v_0)\n",
    "\n",
    "E_t = E_0\n",
    "\n",
    "tolerance = 1e-14\n",
    "\n",
    "# Set the learning rate\n",
    "# Set iterator to 0\n",
    "rate = 0.1\n",
    "iterations = 0\n",
    "\n",
    "while E_t > tolerance:\n",
    "    \n",
    "    # Calculate new weights\n",
    "    u_t1 = u_t - rate * dE_du(u_t, v_t)\n",
    "    v_t1 = v_t - rate * dE_dv(u_t, v_t)\n",
    "    \n",
    "    # Set new weights to time t\n",
    "    u_t = u_t1\n",
    "    v_t = v_t1\n",
    "    \n",
    "    # Calculate new error value in time t\n",
    "    E_t1 = E_uv(u_t, v_t)\n",
    "    \n",
    "    E_t = E_t1\n",
    "    \n",
    "    print ('E_t fot t = {} was {}').format(iterations, E_t)\n",
    "    \n",
    "    iterations += 1\n",
    "    \n",
    "print ('Tolerance reached after {} iterations').format(iterations)\n",
    "print ('The current (u,v) is ({},{})').format(u_t, v_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach the given level of error tolerance after **10** iterations, meaning the correct answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Euclidean plane, if p = (p1, p2) and q = (q1, q2) then the distance is given by\n",
    "\n",
    "${\\displaystyle \\mathrm {d} (\\mathbf {p} ,\\mathbf {q} )={\\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}}}.}$\n",
    "\n",
    "This is equivalent to the Pythagorean theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to a is 1.36571788692\n",
      "Distance to b is 0.668594885774\n",
      "Distance to c is 0.0926123232022\n",
      "Distance to d is 0.127835732282\n",
      "Distance to e is 0.000266921861049\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(u, v, point):\n",
    "    return np.sqrt((u-point[0])**2 + (v-point[1])**2)\n",
    "\n",
    "u_t = 0.0447362903978\n",
    "v_t = 0.0239587140991\n",
    "\n",
    "a = [1, 1]\n",
    "b = [0.713, 0.045]\n",
    "c = [0.016, 0.112]\n",
    "d = [-0.083, 0.029]\n",
    "e = [0.045, 0.024]\n",
    "\n",
    "print ('Distance to a is {}').format(euclidean_distance(u_t, v_t, a))\n",
    "print ('Distance to b is {}').format(euclidean_distance(u_t, v_t, b))\n",
    "print ('Distance to c is {}').format(euclidean_distance(u_t, v_t, c))\n",
    "print ('Distance to d is {}').format(euclidean_distance(u_t, v_t, d))\n",
    "print ('Distance to e is {}').format(euclidean_distance(u_t, v_t, e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the distance to **e** is the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) \n",
    "Now, we will compare the performance of “coordinate descent.” In each iteration,\n",
    "we have two steps along the 2 coordinates. Step 1 is to move only along\n",
    "the u coordinate to reduce the error (assume first-order approximation holds\n",
    "like in gradient descent), and step 2 is to reevaluate and move only along the v\n",
    "coordinate to reduce the error (again, assume first-order approximation holds).\n",
    "Use the same learning rate of η = 0.1 as we did in gradient descent. What will\n",
    "the error E(u, v) be closest to after 15 full iterations (30 steps)?\n",
    "\n",
    "[a] 10−1\n",
    "[b] 10−7\n",
    "[c] 10−14\n",
    "[d] 10−17\n",
    "[e] 10−20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_t for t = 0 was 34.2901631123\n",
      "E_t for t = 1 was 0.534142591372\n",
      "E_t for t = 2 was 0.432660827324\n",
      "E_t for t = 3 was 0.365039735019\n",
      "E_t for t = 4 was 0.31646807536\n",
      "E_t for t = 5 was 0.279763423064\n",
      "E_t for t = 6 was 0.250986311675\n",
      "E_t for t = 7 was 0.227783298944\n",
      "E_t for t = 8 was 0.208656695724\n",
      "E_t for t = 9 was 0.192605658614\n",
      "E_t for t = 10 was 0.178934748408\n",
      "E_t for t = 11 was 0.167145054343\n",
      "E_t for t = 12 was 0.15686898733\n",
      "E_t for t = 13 was 0.147829522524\n",
      "E_t for t = 14 was 0.139813791996\n",
      "Estimated error after 15 iterations: 0.139813791996\n",
      "The current (u,v) is (6.29707589931,-2.85230695408)\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent maafaakkaaas\n",
    "# In this here case we don't explicitly work with weights w(t), but rather u and v\n",
    "\n",
    "# Step 1 - Initialize w(0)\n",
    "\n",
    "u_0 = 1\n",
    "v_0 = 1\n",
    "\n",
    "# We set u and v in time t to this before the first iteration\n",
    "\n",
    "u_t = u_0\n",
    "v_t = v_0\n",
    "\n",
    "# We set the initial error value, and E_t to match this before any iterations of SGD\n",
    "\n",
    "E_0 = E_uv(u_0, v_0)\n",
    "\n",
    "E_t = E_0\n",
    "\n",
    "tolerance = 1e-14\n",
    "\n",
    "# Set the learning rate\n",
    "# Set iterator to 0\n",
    "rate = 0.1\n",
    "iterations = 0\n",
    "\n",
    "while iterations < 15:\n",
    "    \n",
    "    # Calculate new u weight \n",
    "    u_t1 = u_t - rate * dE_du(u_t, v_t)\n",
    "    u_t = u_t1\n",
    "    \n",
    "    # Calculate new v weight and update error\n",
    "    v_t1 = v_t - rate * dE_dv(u_t, v_t)\n",
    "    v_t = v_t1\n",
    "    \n",
    "    E_t1 = E_uv(u_t, v_t)\n",
    "        \n",
    "    E_t = E_t1\n",
    "    \n",
    "    print ('E_t for t = {} was {}').format(iterations, E_t)\n",
    "    \n",
    "    iterations += 1\n",
    "    \n",
    "print ('Estimated error after 15 iterations: {}').format(E_t)\n",
    "print ('The current (u,v) is ({},{})').format(u_t, v_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my calculations, the answer should be **a)**. Barring any misunderstandings regarding the algorithm, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
